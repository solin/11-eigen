%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%

%% Template article for Elsevier's document class `elsarticle'
%% with numbered style bibliographic references
%% SP 2008/03/01
%%
%%
%%
%% $Id: elsarticle-template-num.tex 4 2009-10-24 08:22:58Z rishi $
%%
%%
\documentclass[preprint ,12pt]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times]{elsarticle}
%% \documentclass[final,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,3p,times]{elsarticle}
%% \documentclass[final,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,5p,times]{elsarticle}
%% \documentclass[final,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
\usepackage{amsmath,amssymb,amsfonts,stmaryrd,amsthm}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{pstricks}
\usepackage{eepic}
\usepackage{float}

%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
%\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%
%% \biboptions{comma,round}

% \biboptions{}


\journal{Applied Mathematics and Computation}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}

\newcommand{\cS}{\mathcal{S}}
\newcommand{\tU}{\tilde{U}}
\newcommand{\ta}{\tilde{a}}
\newcommand{\tk}{\tilde{k}}
\newcommand{\bfb}{\mathbf{b}}
\newcommand{\tB}{\tilde{\mathcal{B}}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\calB}{\mathcal{B}}
 \newcommand{\cB}{{B}}
\newcommand{\cL}{\mathcal{L}}
\newcommand{\cK}{\mathcal{K}}
\newcommand{\cE}{\mathcal{E}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cA}{{A}}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\osc}{\mathrm{osc}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\tcb}{\textcolor{blue}}


\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Pavel Solin\corref{cor1}\fnref{label2}}
%% \ead{solin@unr.edu}
%% \ead[url]{http://hpfem.org/\tilde pavel}
%%\fntext[label2]{}
%% \cortext[cor1]{}
%% \address{\fnref{label3}}
%% \fntext[label3]{}

\title{Adaptive Multimesh $hp$-FEM for PDE Eigenproblems}

%% use optional labels to link authors explicitly to addresses:
\author[label1,label2]{Pavel Solin}
\ead{solin@unr.edu}
\author[label3]{Stefano Giani}
\ead{stefano.giani@nottingham.ac.uk}
\address[label1]{Department of Mathematics and Statistics, University of Nevada, Reno, USA}
\address[label2]{Institute of Thermomechanics, Academy of Sciences of the Czech Republic, Prague}
\address[label3]{School of Mathematical Sciences, University of Nottingham, United Kingdom}


%\address{Department of Mathematics and Statistics, University of Nevada, Reno, USA}

\begin{abstract}
We propose an adaptive multimesh higher-order finite element method ($hp$-FEM) 
for efficient solution of eigenproblems originated in partial differential 
equations (PDE). Since one finite element mesh cannot be optimal for several
eigenfunctions simultaneously, we approximate each eigenfunction on an individual 
mesh. The meshes are adapted independently of each other so that 
the overall ratio of approximation error to discrete problem size is minimized.
To avoid notorious problems associated with repeated eigensolver calls during the 
mesh adaptation process, we propose two methods where the eigensolver is called once at the beginning, and 
then an iterative method is employed to converge each eigenvector-eigenvalue pair. \textcolor{red}{Pavel, I changed the previous sentence because one of the method that we are going to describe calls the eigensolver many times and not only on the coarse mesh.}
Automatic $hp$-adaptivity is guided by means of a robust PDE-independent computational 
error estimator that extracts a low-order part from the higher-order approximation. 
The method is described, its theoretical analysis provided, and sample 
applications are presented. Instructions on how to reproduce the results 
are provided. 
\end{abstract}

\begin{keyword}
Partial differential equation \sep Eigenvalue problem \sep Adaptive higher-order finite element
method \sep Multimesh $hp$-FEM
%% keywords here, in the form: keyword \sep keyword
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

%%
%% Start line numbering here if you want
%%
% \linenumbers

%% main text
\section{Introduction}

\begin{itemize}
\item Give examples of applications where accurate approximation of  
      a single eigenfunction is useful.
\item Mention applications where simultaneous approximation of
      several eigenfunctions is needed (I know about DFT. Is there something else?). 
      \textcolor{red}{I could suggest Photonic Crystal problems, it is necessary to compute several eigenvalues in order to identify gaps in the spectrum of the problem}
\item Give references to prior work on adaptive algorithms for 
      PDE eigenproblems, limitations, ... We have to show that we are aware of 
      latest developments.
\end{itemize}


\section{Motivation}

Let us consider a simple eigenproblem of the form 
\begin{equation} \label{one}
-\Delta u = \lambda u
\end{equation}
in a square domain $\Omega = (0, \pi)^2$, equipped with zero Dirichlet boundary conditions 
on the boundary $\partial \Omega$. The eigenvalues $0 < \lambda_1 \le \lambda_2 \le \ldots$ 
are known exactly, and it is of particular interest that $\lambda_2 = \lambda_3 = 5$ and 
$\lambda_5 = \lambda_6 = 10$. We discretize (\ref{one}) via the finite element method
on a equidistant Cartesian mesh consisting of 16 quadratic elements as shown in Fig. \ref{fig:mesh1}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.4\textwidth]{img/mesh_1.png}
\end{center}
\vspace{-5mm}
\caption{Mesh before the first call to the generalized eigensolver.}
\label{fig:mesh1}
\end{figure}

Approximate eigenfunctions for $\lambda_2, \lambda_3$ and $\lambda_5, \lambda_6$
are shown in the first and second row of Fig. \ref{fig:eigen1}, respectively.
The reader can see that the eigenfunctions exhibit anisotropies that make it 
impossible for a single finite element mesh to be optimal for all 
of them simultaneously.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.4\textwidth]{img/eigen_1.png}
\includegraphics[width=0.4\textwidth]{img/eigen_2.png}\\
\includegraphics[width=0.4\textwidth]{img/eigen_3.png}
\includegraphics[width=0.4\textwidth]{img/eigen_4.png}\\
\end{center}
\vspace{-5mm}
\caption{Approximate eigenfunctions for $\lambda_2, \lambda_3$ and 
$\lambda_5, \lambda_6$ after the first call to the generalized eigensolver.}
\label{fig:eigen1}
\end{figure}

Let us assume that by means of some mesh adaptation algorithm, the mesh undergoes 
$p$-refinement of the four interior elements, whose new polynomial degree 
will be $p = 3$, as shown in Fig. \ref{fig:mesh2}.

\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.4\textwidth]{img/mesh_2.png}
\end{center}
\vspace{-5mm}
\caption{Mesh before the second call to the generalized eigensolver.}
\label{fig:mesh2}
\end{figure}

After refining the mesh in this way, the eigensolver is called again. 
The resulting approximate eigenfunctions for $\lambda_2, \lambda_3$ and $\lambda_5, \lambda_6$
are shown in the first and second row of Fig. \ref{fig:eigen2}, respectively.
The reader can see that now we lost track of all four eigenfunctions 
that we calculated during the first eigensolver call. 




\begin{figure}[!ht]
\begin{center}
\includegraphics[width=0.4\textwidth]{img/eigen_5.png}
\includegraphics[width=0.4\textwidth]{img/eigen_6.png}\\
\includegraphics[width=0.4\textwidth]{img/eigen_7.png}
\includegraphics[width=0.4\textwidth]{img/eigen_8.png}\\
\end{center}
\vspace{-5mm}
\caption{Approximate eigenfunctions for $\lambda_2, \lambda_3$ and 
$\lambda_5, \lambda_6$ after the second call to the generalized eigensolver.}
\label{fig:eigen2}
\end{figure}

After a third call to the generalized eigensolver, the order of the 
eigenfunctions in each pair is switched. Instructions on how to reproduce 
these results are provided in Section \ref{sec:reproducibility}.

Unfortunately, the current 
state-of-the-art of generalized eigensolvers does not offer any known
way to circumvent these problems [REFERENCE]. The only solution known 
to us is to avoid repeated eigensolver calls. In this paper we describe 
a method that only calls an eigensolver once, and then employs an iterative 
algorithm to perform automatic mesh adaptation. We use this method to 
design an algorithm that approximates efficiently each eigenfunction on 
an individual mesh. \textcolor{red}{The reconstruction technology can be used to solve this problem also when an eigensolver is called repeated times. We need to decide how to present this point.}

Let us begin with describing the adaptive $hp$-FEM algorithm
that we use for eigenfunctions associated with eigenvalues
of multiplicity one.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preliminaries}

Throughout $L^2(\Omega)$
denotes the usual space of square integrable real valued functions
equipped with the standard norm
\begin{equation}\label{eq:l2}
\|f\|_{0}\ :=\ \int_\Omega  |f|^2\ .
\end{equation}
$H^1(\Omega)$ denotes the usual space of functions in $L^2(\Omega)$
with square integrable gradient, with  $H^1$-norm denoted $\|f\|_1$.

The variational formulation of problem \eqref{one} is:
\emph{seek eigenpairs of the form $(\lambda_j,u_j)\in
\mathbb{R}\times H^1_0(\Omega)$
such that}
\begin{equation}
\label{eq:var_prob}
\left.
\begin{array}{lcl}
a(u_j,v)&=& \lambda_j\ b(u_j,v)\ ,
\quad \text{for all } \quad v  \in H^1_0(\Omega)\\
 \Vert u_j \Vert_{0} &=& 1
\end{array}\quad
\right\}
\end{equation}
where
\begin{equation}\label{eq:a}
a(u,v):=\int_\Omega \nabla u(x)\cdot \nabla v(x)\ ,
\end{equation}
and
\begin{equation}\label{eq:b}
b(u,v):=\int_\Omega u(x) v(x)\ .
\end{equation}

Now, to discretize (\ref{eq:var_prob}), let $\cT_n\ , n =
1,2,\ldots $ denote a family of 1-irregular meshes on $\Omega$ composed by possible triangular and quadrilateral elements. 
These meshes may be computed adaptively. 
With  $h_{n,\tau}$ we denote  the diameter of element $\tau$,  
we define
$
h_n:=\max_{\tau\in \mathcal{T}_n}\{h_{n,\tau}\}.
$
Similarly with  $p_{n,\tau}$ we denote  the order of polynomials of element $\tau$,  
we define
$
p_n:=\min_{\tau\in \mathcal{T}_n}\{p_{n,\tau}\}.
$
On any mesh $\mathcal{T}_n$ we denote by $V_n \subset H^1_0(\Omega)$ the finite
dimensional space of continuous functions $v$ such that on any element $\tau$ we have that 
$v|_\tau\in \mathcal{P}_{p_{n,\tau}}(K)$, where either $\mathcal{P}_{p_{n,\tau}}(K)$ is the space of polynomials of total degree at most $p_{n,\tau}$ if $\tau$ is a triangular element or $\mathcal{P}_{p_{n,\tau}}(K)$ is the space of polynomials of degree at most $p_{n,\tau}$ in each variable if $\tau$ is a quadrilateral element.


%\textcolor{red}{I have to define better the meshes and the space and add references.}

The discrete version of \eqref{eq:var_prob} is:
\emph{seek eigenpairs of the form $(\lambda_{j,n},u_{j,n})\in
\mathbb{R}\times V_n$
such that}
\begin{equation}
\label{eq:disc_prob}
\left.
\begin{array}{lcl}
a(u_{jn},v_{n})&=& \lambda_{j,n}\ b(u_{j,n},v_{n})\ ,
\quad \text{for all } \quad v_{n}  \in V_n\\
 \Vert u_{j,n} \Vert_{0} &=& 1
\end{array}\quad
\right\}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Picard's method}

Problem \eqref{eq:disc_prob} can be reformulated in matrix format as:
\emph{seek eigenpairs of the form $(\lambda,\mathbf{u})\in
\mathbb{R}\times \mathbb{R}^N$, where $N$ is the dimension of $V_n$,
such that}
\begin{equation}
\label{eq:disc_prob_mat}
\left.
\begin{array}{lcl}
\mathbf{A} \mathbf{u}&=& \lambda\mathbf{B}\mathbf{u}\ ,
\\
\mathbf{u}^t\mathbf{B} \mathbf{u} &=& 1
\end{array}\quad
\right\}
\end{equation}
where the entries of the matrices $\mathbf{A}$ and $\mathbf{B}$ are 
$$
\mathbf{A}_{k,p}:=a(\phi_k,\phi_p)\ ,\quad\mathbf{B}_{k,p}:=b(\phi_k,\phi_p)\ .
$$


The Picard's method, see Algorithm~\ref{alg:picard}, takes as arguments the matrices $\mathbf{A}$ and $\mathbf{B}$, an initial guess $\tilde u$ for the eigenfunction and a tolerance $\mathrm{Tol}$. The algorithm returns an approximated eigenpair $(\lambda_{n},u_{n})$.
Because we use this iterative method on a sequence of adaptively refined meshes, we normally set as initial guess
the projection in the refined mesh of the eigenfunction of interest $u_{j,n-1}$.

\begin{algorithm}[H] \caption{Picard's method} \label{alg:picard} 
\begin{algorithmic}

\STATE{$(\lambda_{j,n},u_{j,n}):=\mathrm{Picard}
    (\mathbf{A}, \mathbf{B},\tilde u,\mathrm{Tol})$}

\STATE{$\mathbf{u}^1:=\tilde u$}
\STATE{$\displaystyle\lambda^{1}:=\frac{u_{j,n}^t\mathbf{A}u_{j,n}}{u_{j,n}^t\mathbf{B}u_{j,n}}$}
\STATE{$m=1$}
\REPEAT

\STATE{$\mathbf{u}^{m+1}:=\mathbf{A}^{-1}\lambda^m\mathbf{B}\mathbf{u}^{m}$}
\STATE{$\displaystyle\lambda^{m+1}:=\frac{(\mathbf{u}^{m+1})^t\mathbf{A}\mathbf{u}^{m+1}}{(\mathbf{u}^{m+1})^t\mathbf{B}\mathbf{u}^{m+1}}$}
\STATE{$m:=m+1$}
\UNTIL{ Can you fill in the details of the stop criteria?}
\STATE{$u_{n}:=\mathbf{u}^{m}$}
\STATE{$\lambda_{n}:=\lambda^m$}
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}\label{lm:picard_b}
The Picard's method in exact arithmetic conserves the norm of the vectors, i.e. for any $m\ge 1$,
$$
(\mathbf{u}^{m})^t\mathbf{B}\mathbf{u}^{m}=(\mathbf{u}^{m-1})^t\mathbf{B}\mathbf{u}^{m-1}\ .
$$
\end{lemma}

\begin{proof}
Using the definition of the discrete problem $\mathbf{A}\mathbf{u}=\lambda \mathbf{B}\mathbf{u}$, we have:
$$
(\mathbf{u}^{m+1})^t\mathbf{B}\mathbf{u}^{m+1}=\lambda^m(\mathbf{u}^{m+1})^t\mathbf{B}\mathbf{A}^{-1}\mathbf{B}\mathbf{u}^{m}
=(\lambda^m)^2\mathbf{A}^{-t}\mathbf{B}^t(\mathbf{u}^{m})t\mathbf{B}\mathbf{A}^{-1}\mathbf{B}\mathbf{u}^{m}
=(\mathbf{u}^{m})^t\mathbf{B}\mathbf{u}^{m}\ .
$$
\end{proof}

From Lemma~\ref{lm:picard_b} it is clear that the Picard's method, in comparison to other iterative methods like the power method and the inverse iteration, doesn't need a normalization step to prevent any underflow or overflow.

The next theorem shows that the Picard's method always converges to the smallest eigenvalue.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{theorem}\label{th:picard_conv}
The Picard's method in exact arithmetic converges into the eigenspace which is not orthogonal to the initial guess $\mathbf{u}^1$ and whose eigenvalue has minimum module.
\end{theorem}

\begin{proof}
Any vector $\mathbf{u}^m$ can be expressed as 
$$
\mathbf{u}^m=\sum_{i=1}^N c_i^m \mathbf{u}_i\ ,
$$
where $c_i^m$ are real coefficients, $N$ is the size of the matrices $\mathbf{A}$ and $\mathbf{B}$ and the vectors $\mathbf{u}_i\equiv u_{i,n}$ are the eigenvectors of the discrete problem, which form an orthonormal basis.
With no lost in generality we can assume that $\lambda_1$ is the eigenvalue of minimum module and that $c_1^1$ is different from 0.

In the case that $\lambda_1$ is simple we have from the definition of the problem:
$$
\mathbf{u}^{m+1}=\mathbf{A}^{-1}\lambda^m\mathbf{B}\mathbf{u}^{m}
=\Big(\Pi_{j=1}^m\lambda^{j}\Big)\Big(\mathbf{A}^{-1}\mathbf{B}\Big)^m\mathbf{u}^1
=\Big(\Pi_{j=1}^m\lambda^{j}\Big)\sum_{i=1}^N c_i^1 (\lambda_i)^{-m}\mathbf{u}_i\ ,
$$
where $\lambda_i$ are the eigenvalues corresponding to $\mathbf{u}_i$.
Then
$$
\mathbf{u}^{m+1}=\Big(\Pi_{j=1}^m\lambda^{j}\Big)(\lambda_1)^{-m}\Big( c_1^1 \mathbf{u}_1 +
\sum_{i=2}^N c_i^1\frac{(\lambda_1)^m}{(\lambda_i)^{m}}\mathbf{u}_i\Big) \ .
$$
By simply multiplying both sides by $u_1$, we have
$$
c_1^{m+1}=\Big(\Pi_{j=1}^m\lambda^{j}\Big)(\lambda_1)^{-m}c_1^1\ ,
$$
and more in general for any $u_i$, $i>1$, we have
$$
c_i^{m+1}=\Big(\Pi_{j=1}^m\lambda^{j}\Big)(\lambda_1)^{-m}\frac{(\lambda_1)^m}{(\lambda_i)^{m}}c_i^1\ .
$$
So for each $i>1$ we have:
$$
\frac{c_1^{m+1}}{c_i^{m+1}}=\frac{(\lambda_i)^m}{(\lambda_1)^{m}}\frac{c_1^1}{c_i^1}\ ,
$$
and since $|\lambda_i/\lambda_1|>1$ we have that
\begin{equation}\label{eq:picard_conv_1}
\lim_{m\rightarrow \infty}\Big\vert\frac{c_1^{m+1}}{c_i^{m+1}}\Big\vert= \infty\ .
\end{equation}
From Lemma~\ref{lm:picard_b} we have that
$$
\sum_{i=1}^N (c_i^{m+1})^2 = (\mathbf{u}^{m+1})^t\mathbf{B}\mathbf{u}^{m+1}
=(\mathbf{u}^1)^t\mathbf{B}\mathbf{u}^1=\sum_{i=1}^N (c_i^1)^2\ ,
$$
and so from \eqref{eq:picard_conv_1} we can conclude that $\lim_{m\rightarrow \infty}|c_i^{m+1}|=0$, for all $i>1$, which is equivalent to say that $\lim_{m\rightarrow \infty}\mathbf{u}^{m+1}\in \mathrm{span}\{\mathbf{u}_1\}$.

In the case that $\lambda_1$ has multiplicity $R$ and that $c_r^1$, for some $1\leq r\leq R$, is not zero,
we similarly have that for all $i>R$:
$$
\lim_{m\rightarrow \infty}\Big\vert\frac{c_r^{m+1}}{c_i^{m+1}}\Big\vert= \infty\ ,
$$
and so $\lim_{m\rightarrow \infty}|c_i^{m+1}|=0$, which proves that $\lim_{m\rightarrow \infty}\mathbf{u}^{m+1}\in \mathrm{span}\{\mathbf{u}_1,\dots,\mathbf{u}_R\}$.

\end{proof}

Theorem~\ref{th:picard_conv} shows that even if the initial guess $\mathbf{u}^1$ is very close to a certain discrete eigenfunction $u_{i,n}$, for some $i$, the method can always converges to a different eigenfunction or a linear combinations of eigenfunctions with corresponding eigenvalues smaller in module than $\lambda_{i,n}$. In real arithmetic, even if the initial guess $\mathbf{u}^1$ is orthogonal to all eigenfunctions of indexes less than $i$, for some $m>1$ the orthogonality could be perturbed, due to round-off errors, and the method can eventually converges anyway to a different eigenfunction or a linear combinations of eigenfunctions with corresponding eigenvalues smaller in module than $\lambda_{i,n}$.

\textcolor{red}{Pavel, can you put here a figure like Figure~\ref{fig:eigen1} where we show that the plain Picard's method converges to the wrong eigenfunction?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Picard's method with orthogonalization}

In order to make the Picard's method suitable to approximate efficiently any discrete eigenpair, and not only the first one, we derived Algorithm~\ref{alg:picard_ortho}, which has an orthogonalization procedure in it.

The Picard's method with orthogonalization takes as arguments the matrices $\mathbf{A}$ and $\mathbf{B}$ of \eqref{eq:disc_prob}, an initial guess $\tilde u$ for the eigenfunction, a tolerance $\mathrm{Tol}$and it also takes the $j-1$ eigenfunctions $u_{1,n},\dots,u_{j-1,n}$.
Then it returns the eigenpair $\lambda_{j,n},u_{j,n}$. 
Then it returns the eigenpair $\lambda_{j,n},u_{j,n}$ on the refined mesh.

This method never converges to an eigenfunction of index smaller than $j$ because for any $m\ge 1$, the vector $\mathbf{u}^m$ is orthogonal to all eigenfunctions $u_{1,n},\dots,u_{j-1,n}$, i.e. all coefficients 
$c_1^m,\dots,c_{j-1}^m$ in the expansion of $\mathbf{u}^m$ are zero, so the eigenvalue smallest in module is $\lambda_j$ and the Picard's method naturally converges to it.

%Anyway this is not enough to guarantee to not lose the eigenfunction that we want because if a multiple eigenspace splits differently due to the refinement of the mesh, the eigenfunction of the refined mesh are not similar to the wanted eigenfunction on the coarse mesh.

%\textcolor{red}{Pavel, we should try to find such an example.}

\begin{algorithm}[H] \caption{Picard's method with orthogonalization} \label{alg:picard_ortho} 
\begin{algorithmic}

\STATE{$(\lambda_{j,n},u_{j,n}):=\mathrm{PicardOrtho}
    (\mathbf{A}, \mathbf{B},\tilde u_{j,n-1},\mathrm{Tol},u_{1,n},\dots
    ,u_{j-1,n})$}
    

\STATE{$\mathbf{u}^1:=\tilde u_{j,n-1}$}
\STATE{$\displaystyle\lambda^{1}:=\frac{u_{j,n}^t\mathbf{A}u_{j,n}}{u_{j,n}^t\mathbf{B}u_{j,n}}$}
\STATE{$m=1$}
\REPEAT

\STATE{$\mathbf{u}^{m+1}:=\mathbf{A}^{-1}\lambda^m\mathbf{B}\mathbf{u}^{m}$}


\FOR{$i = 1$ to $j-1$} 
\STATE $\mathbf{u}^{m+1}:=\mathbf{u}^{m+1}-(u_{i,n}^t\mathbf{B}\mathbf{u}^{m+1})u_{i,n}$
\COMMENT{Orthogonalization}
\ENDFOR


\STATE $\displaystyle \mathbf{u}^{m+1}=\frac{\mathbf{u}^{m+1}}{((\mathbf{u}^{m+1})^t\mathbf{B}\mathbf{u}^{m+1})^{1/2}}$
\COMMENT{Normalize}
\STATE{$\displaystyle\lambda^{m+1}:=\frac{(\mathbf{u}^{m+1})^t\mathbf{A}\mathbf{u}^{m+1}}{(\mathbf{u}^{m+1})^t\mathbf{B}\mathbf{u}^{m+1}}$}
\STATE{$m:=m+1$}
\UNTIL{ Can you fill in the details of the stop criteria?}
\STATE{$u_{j,n}:=\mathbf{u}^{m}$}
\STATE{$\lambda_{j,n}:=\lambda^m$}
\end{algorithmic}
\end{algorithm}

As can be seen in Algorithm~\ref{alg:picard_ortho}  the orthogonalization is done at each iteration, this is necessary in real arithmetic to guarantees that $\mathbf{u}^m$ is orthogonal to all eigenfunctions $u_{1,n},\dots,u_{j-1,n}$, for all $m$. Otherwise in exact arithmetic it would be enough to orthogonalize only $\mathbf{u}^1$. Moreover a normalization step is necessary in all iterations because due to the orthogonalization procedure, this version of the Picard's method does not conserve the norm of the vectors and possible underflows or overflows could happen with no normalization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Newton's method with orthogonalization}

The second iterative method that we are going to propose is based on the Newton's method applied to eigenvalue problems. Denoting by $\tilde x:=(x,\lambda)$, we have that problem \eqref{eq:var_prob} can be rewritten in the form
$$
0=f(\tilde x):=
\left(
\begin{array}{lcl}
A x&-& \lambda_j\ Bx
\\
  x^T Bx&-& 1
\end{array}\quad
\right) ,
$$
then denoting by $\tilde h:=(h, \delta)$ the increment, we have that the truncated Taylor series of the problem is
\begin{equation}\label{eq:newton}
f(\tilde x + \tilde h)\approx f(\tilde x) + J_f(\tilde x)\cdot \tilde h\ , 
\end{equation}
where the Jacobian matrix is defined as
$$
J_f(\tilde x):=
\left(
\begin{array}{lr}
A - B\lambda & -Bx
\\
  2Bx^T  & 0
\end{array}\quad
\right) .
$$

Then when $\tilde x + \tilde h$ is a solution of \eqref{eq:var_prob}, we have from \eqref{eq:newton}
that 
$$
J_f(\tilde x)\cdot \tilde h = - f(\tilde x)\ ,
$$
which defines the linear problem of the Newton's method that we are solving.
\begin{algorithm}[H] \caption{Newton's method} \label{alg:newton} 
\begin{algorithmic}

\STATE{$(\lambda_{j,n},u_{j,n}):=\mathrm{Newton}
    (\mathbf{A}, \mathbf{B},\tilde u,\mathrm{Tol})$}

\STATE{$\mathbf{u}^1:=\tilde u$}
\STATE{$\displaystyle\lambda^{1}:=\frac{u_{j,n}^t\mathbf{A}u_{j,n}}{u_{j,n}^t\mathbf{B}u_{j,n}}$}
\STATE{$m=1$}
\REPEAT

\STATE{Solve $J_f(\mathbf{u}^m,\lambda^m)\cdot \tilde h = - f(\mathbf{u}^m,\lambda^m)$}
\STATE{$\mathbf{u}^{m+1}:=\mathbf{u}^m+h$}
\STATE{$\lambda^{m+1}:=\lambda^m+\delta$}
\STATE{$m:=m+1$}
\UNTIL{ Can you fill in the details of the stop criteria?}
\STATE{$u_{n}:=\mathbf{u}^{m}$}
\STATE{$\lambda_{n}:=\lambda^m$}
\end{algorithmic}
\end{algorithm}

In order to make the method suitable for all eigenpairs, we are going write a version of the Newton's method that uses an orthogonalization procedure, similarly to what we have already done for the Picard's method.
\begin{algorithm}[H] \caption{Newton's method with orthogonalization} \label{alg:newton_ortho} 
\begin{algorithmic}

\STATE{$(\lambda_{j,n},u_{j,n}):=\mathrm{NewtonOrtho}
    (\mathbf{A}, \mathbf{B},\tilde u_{j,n-1},\mathrm{Tol},u_{1,n},\dots
    ,u_{j-1,n})$}
    

\STATE{$\mathbf{u}^1:=\tilde u_{j,n-1}$}
\STATE{$\displaystyle\lambda^{1}:=\frac{u_{j,n}^t\mathbf{A}u_{j,n}}{u_{j,n}^t\mathbf{B}u_{j,n}}$}
\STATE{$m=1$}
\REPEAT

%\STATE{$\mathbf{u}^{m+1}:=\mathbf{A}^{-1}\lambda^m\mathbf{B}\mathbf{u}^{m}$}
\STATE{Solve $J_f(\mathbf{u}^m,\lambda^m)\cdot \tilde h = - f(\mathbf{u}^m,\lambda^m)$}
\STATE{$\mathbf{u}^{m+1}:=\mathbf{u}^m+h$}
\STATE{$\lambda^{m+1}:=\lambda^m+\delta$}
\STATE{$m:=m+1$}


\FOR{$i = 1$ to $j-1$} 
\STATE $\mathbf{u}^{m+1}:=\mathbf{u}^{m+1}-(u_{i,n}^t\mathbf{B}\mathbf{u}^{m+1})u_{i,n}$
\COMMENT{Orthogonalization}
\ENDFOR


\STATE $\displaystyle \mathbf{u}^{m+1}=\frac{\mathbf{u}^{m+1}}{((\mathbf{u}^{m+1})^t\mathbf{B}\mathbf{u}^{m+1})^{1/2}}$
\COMMENT{Normalize}
\STATE{$\displaystyle\lambda^{m+1}:=\frac{(\mathbf{u}^{m+1})^t\mathbf{A}\mathbf{u}^{m+1}}{(\mathbf{u}^{m+1})^t\mathbf{B}\mathbf{u}^{m+1}}$}
\STATE{$m:=m+1$}
\UNTIL{ Can you fill in the details of the stop criteria?}
\STATE{$u_{j,n}:=\mathbf{u}^{m}$}
\STATE{$\lambda_{j,n}:=\lambda^m$}
\end{algorithmic}
\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Reconstruction technology}

It is well known that the discretization process perturbs the spectrum, in particular the eigenspace $E(\lambda_j)$ of multiple eigenvalue $\lambda_j$ can be split in more than one discrete eigenspace $E(\lambda_{j,n}),E(\lambda_{j+1,n}),\dots,E(\lambda_{j+m,n})$ with correspondent discrete eigenvalues $\lambda_{j,n},\lambda_{j+1,n},\dots,\lambda_{j+m,n}$ forming a small cluster for sufficiently rich finite element space, also under the same assumption we have that
$$
\mathrm{dim}\ E(\lambda_j)=\sum_{i=0}^m\mathrm{dim}\ E(\lambda_{j+i,n})\ .
$$
This phenomenon is already well documented in literature,  see \cite{strang, babuska, hackbusch}.

Different finite element spaces can split the same multiple eigenspace in different ways, this also happens with adaptively refined meshes. It is not rare that the same multiple eigenspace is split differently on the coarse and on the refined meshes. A different split corresponds to different discrete eigenfunctions, then it is not always possible to find for the same eigenvalue on the refined mesh an eigenfunction similar to the one on the coarse mesh.

\textcolor{red}{Pavel, I think we need another figure here. It would be easy to see the phenomenon on unstructured meshes.}

We propose a way to always construct on a refined mesh, an approximation of the same eigenfunction as on the coarse mesh. The idea is based on the fact that for a sufficiently rich finite element space, the space $M_n(\lambda_j)=\mathrm{span}\{E(\lambda_{j,n}),E(\lambda_{j+1,n}),\dots E(\lambda_{j+m,n})\}$ is an approximation of the space $E(\lambda_j)$, see \cite{strang}. Let us denote the space $M_{n,1}(\lambda_j)$ as the subspace of $M_n(\lambda_j)$ of functions with unit norm in the $L^2$.
So for any function $U_{n-1}\in M_{n-1,1}(\lambda_j)$, we propose the function $U_{n}\in M_{n,1}(\lambda_j)$ that minimize the $\|U_{n-1}-U_{n}\|_{0,\Omega}$ as an approximation of $U_{n-1}$ on the refined mesh. For a sufficiently rich finite element space the minimizer is unique. By construction
\begin{equation}\label{eq:const}
U_n=\sum_{i=1}^{R} c_i \ u_{i,n}\ ,
\end{equation}
where $u_{1,n},u_{2,n},\dots,u_{R,n}$, with $R=\mathrm{dim}\ E(\lambda_j)$, are eigenfunctions of the discrete problem forming  an orthonormal basis for
$M_{n,1}$ and where the coefficients $c_i$ satisfy 
\begin{equation}\label{eq:cond_on_corf}
\sum_{i=1}^{R} c_i^2=1\ .
\end{equation}

Form the definition of problem \eqref{eq:var_prob} we have that the reconstructed eigenvalue is defined as
$$
\Lambda_n=\frac{a(U_n,U_n)}{b(U_n,U_n)}\ .
$$

The couple $(\Lambda_n,U_n)$ is not a discrete eigenpair of problem \eqref{eq:var_prob} in general, in section~\ref{sse:pcf_priori} we prove that $(\Lambda_n,U_n)$ converges a priori at the same rate as any other discrete eigenpair of \eqref{eq:var_prob} to a continuous eigenpair.
\textcolor{red}{Pavel, the point that $(\Lambda_n,U_n)$ is not a discrete eigenpair is quite important and it is one of the new contribution in this paper, do you think that it has been stressed enough?}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computing Reference Solution via Reconstruction}

In this section we present three algorithms to compute approximations of eigenpairs. Each algorithm is based on a different method to compute the discrete spectrum, but all of them use the reconstruction technology to keep track of the eigenfunction of interest.

In all algorithms we are going to use call an iterative eigensolver with calling interface
$\{(\lambda_{j,n},u_{j,n})_{j=1}^{i}\}:=\mathrm{Eigensolver}(\mathbf{A},\mathbf{B},s,i,\mathrm{Tol},\mathrm{MaxIter})$, that computes the set of discrete eigenpairs $\{(\lambda_{j,n},u_{j,n})\}_{j=1}^{i}$ and where $\mathbf{A}$ is the stiffness matrix of the problem, $\mathbf{B}$ is the mass matrix of the problem $s$ is a value in the vicinity of the first eigenvalue (without loss in generality we can assume that $s=0$), $i$ is the number of eigenpairs to compute, $\mathrm{Tol}$ is the requested tolerance for the eigenpairs and $\mathrm{MaxIter}$ is the maximum number of iterations. 


All algorithm we describe below are based on the reconstruction technology which is guided by two parameters: $\mathrm{DTE}$ and $\mathrm{FIE}$. The parameter $\mathrm{DTE}$ should be equal to the multiplicity of the continuous eigenvalue $\lambda$ that the user want to approximate. All the algorithm works also when $\mathrm{DTE}$ contains an upper bound of the multiplicity of $\lambda$, so in practise the multiplicity of the target eigenvalue is not necessary to be known exactly. The parameter $\mathrm{FIE}$ should be equal to the index $i$ of the first discrete eigenvalue on the initial mesh $\lambda_{i,0}$ that approximates $\lambda$. The reconstruction technology is described in Algorithm~\eqref{alg:reconstruction}.

\begin{algorithm}[H] \caption{Reconstruction algorithm} \label{alg:reconstruction} 
\begin{algorithmic}

\STATE{$(\Lambda_n,U_n):=\mathrm{Reconstruction}
    (\{(\lambda_{\mathrm{FIE}+j,n},u_{\mathrm{FIE}+j,n})\}_{j=0}^{\mathrm{FIE}+\mathrm{DTE}-1},
(\Lambda_{n-1},U_{n-1}))$}

\STATE{Compute $\displaystyle (\Lambda_n,U_n):=\sum_{i=\mathrm{FIE}}^{\mathrm{FIE}+\mathrm{DTE}-1}
b(u_{i,n},U_{n-1})u_{i,n}$}

\STATE{$\displaystyle U_n:=\frac{U_n}{\sqrt{b(U_n,U_n)}}$}
\COMMENT{Normalize}
\STATE{$\Lambda_n:=a(U_n,U_n)$}

\end{algorithmic}
\end{algorithm}

The first method is based on the iterative eigensolver. The only three parameters not yet defined are $M$ which is the maximum number of mesh adaptation requested,$0<\mathrm{FIE}\mathrm{TE}\leq\mathrm{FIE}+\mathrm{DTE} $ which is the index of the eigenvalue that the user want to target and $\mathrm{err}$ which is tolerance for the residual.

\begin{algorithm}[H] \caption{Adaptive method based on the iterative eigensolver} \label{alg:eigsolver_adapt} 
\begin{algorithmic}

\STATE{$(\Lambda_M,U_M):=\mathrm{EigensolverAdapt}
    (\mathcal{T}_0, V_0,M,\mathrm{err},\mathrm{Tol},\mathrm{MaxIter}
,\mathrm{DTE},\mathrm{FIE},\mathrm{TE})$}

\STATE{Construct $\mathbf{A}_0$ and $\mathbf{B}_0$}

\STATE{$\{(\lambda_{j,0},u_{j,0})\}_{j=1}^{\mathrm{DTE}+\mathrm{FIE}-1}:=\mathrm{Eigensolver}(\mathbf{A}_0,\mathbf{B}_0,0,\mathrm{DTE}+\mathrm{FIE}-1,$}
\STATE{$\quad\quad\quad\mathrm{Tol},\mathrm{MaxIter})$}
\STATE{$(\Lambda_0,U_0):=(\lambda_{\mathrm{TE},0},u_{\mathrm{TE},0})$}
\STATE{Compute the residual $\eta$ for the couple $(\Lambda_0,U_0)$}
\STATE{$m:=1$}
\REPEAT
\STATE{Construct $\mathbf{A}_m$ and $\mathbf{B}_m$}
\STATE{Construct the mesh $\mathcal{T}_m$ and the finite element space $V_m$ adapting $\mathcal{T}_{m-1}$ and $V_{m-1}$}
\STATE{$\{(\lambda_{j,m},u_{j,m})\}_{j=1}^{\mathrm{DTE}+\mathrm{FIE}-1}:=\mathrm{Eigensolver}(\mathbf{A}_m,\mathbf{B}_m,0,
\mathrm{DTE}+\mathrm{FIE}-1,$}
\STATE{$\quad\quad\quad\mathrm{Tol},\mathrm{MaxIter})$}
\STATE{$(\Lambda_m,U_m):=\mathrm{Reconstruction}
    (\{(\lambda_{\mathrm{DTE}+j,m},u_{\mathrm{DTE}+j,m})\}_{j=0}^{\mathrm{FIE}-1},
(\Lambda_{m-1},U_{m-1}))$}
\STATE{Compute the residual $\eta$ for the couple $(\Lambda_m,U_m)$}
\STATE{$m:=m+1$}
\UNTIL{ $m\leq M$ OR $\eta \leq \mathrm{err}$}
\end{algorithmic}
\end{algorithm}

Similarly we define the adaptive methods based on the Picard's and Newton's method.

\begin{algorithm}[H] \caption{Adaptive method based on the Picard's method} \label{alg:picard_adapt} 
\begin{algorithmic}

\STATE{$(\Lambda_M,U_M):=\mathrm{PicardAdapt}
    (\mathcal{T}_0, V_0,M,\mathrm{err},\mathrm{Tol},\mathrm{MaxIter}
,\mathrm{DTE},\mathrm{FIE},\mathrm{TE})$}

\STATE{Construct $\mathbf{A}_0$ and $\mathbf{B}_0$}

\STATE{$\{(\lambda_{j,0},u_{j,0})\}_{j=1}^{\mathrm{DTE}+\mathrm{FIE}-1}:=\mathrm{Eigensolver}(\mathbf{A}_0,\mathbf{B}_0,0,
\mathrm{DTE}+\mathrm{FIE}-1,$}
\STATE{$\quad\quad\quad\mathrm{Tol},\mathrm{MaxIter})$}
\STATE{$(\Lambda_0,U_0):=(\lambda_{\mathrm{TE},0},u_{\mathrm{TE},0})$}
\STATE{Compute the residual $\eta$ for the couple $(\Lambda_0,U_0)$}
\STATE{$m:=1$}
\REPEAT
\STATE{Construct $\mathbf{A}_m$ and $\mathbf{B}_m$}
\STATE{Construct the mesh $\mathcal{T}_m$ and the finite element space $V_m$ adapting $\mathcal{T}_{m-1}$ and $V_{m-1}$}
\STATE{$(\lambda_{0,m},u_{0,m}):=\mathrm{Picard}
    (\mathbf{A}_m, \mathbf{B}_m,u_{m-1},\mathrm{Tol})$}
\STATE{$j=1$}
\FOR{$j = 1$ to $\mathrm{DTE}+\mathrm{FIE}-1$}

\STATE{$(\lambda_{j,m},u_{j,m}):=\mathrm{PicardOrtho}
    (\mathbf{A}_m, \mathbf{B}_m,u_{0,m-1},\mathrm{Tol},u_{0,n},\dots
    ,u_{j-1,m-1})$}


\ENDFOR
\STATE{$(\Lambda_m,U_m):=\mathrm{Reconstruction}
    (\{(\lambda_{\mathrm{DTE}+j,m},u_{\mathrm{DTE}+j,m})\}_{j=0}^{\mathrm{FIE}-1},
(\Lambda_{m-1},U_{m-1}))$}
\STATE{Compute the residual $\eta$ for the couple $(\Lambda_m,U_m)$}
\STATE{$m:=m+1$}
\UNTIL{ $m\leq M$ OR $\eta \leq \mathrm{err}$}
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] \caption{Adaptive method based on the Newton's method} \label{alg:newton_adapt} 
\begin{algorithmic}

\STATE{$(\Lambda_M,U_M):=\mathrm{NewtonAdapt}
    (\mathcal{T}_0, V_0,M,\mathrm{err},\mathrm{Tol},\mathrm{MaxIter}
,\mathrm{DTE},\mathrm{FIE},\mathrm{TE})$}

\STATE{Construct $\mathbf{A}_0$ and $\mathbf{B}_0$}

\STATE{$\{(\lambda_{j,0},u_{j,0})\}_{j=1}^{\mathrm{DTE}+\mathrm{FIE}-1}:=\mathrm{Eigensolver}(\mathbf{A}_0,\mathbf{B}_0,0,
\mathrm{DTE}+\mathrm{FIE}-1,$}
\STATE{$\quad\quad\quad\mathrm{Tol},\mathrm{MaxIter})$}
\STATE{$(\Lambda_0,U_0):=(\lambda_{\mathrm{TE},0},u_{\mathrm{TE},0})$}
\STATE{Compute the residual $\eta$ for the couple $(\Lambda_0,U_0)$}
\STATE{$m:=1$}
\REPEAT
\STATE{Construct $\mathbf{A}_m$ and $\mathbf{B}_m$}
\STATE{Construct the mesh $\mathcal{T}_m$ and the finite element space $V_m$ adapting $\mathcal{T}_{m-1}$ and $V_{m-1}$}
\STATE{$(\lambda_{0,m},u_{0,m}):=\mathrm{Newton}
    (\mathbf{A}_m, \mathbf{B}_m,u_{m-1},\mathrm{Tol})$}
\STATE{$j=1$}
\FOR{$j = 1$ to $\mathrm{DTE}+\mathrm{FIE}-1$}

\STATE{$(\lambda_{j,m},u_{j,m}):=\mathrm{NewtonOrtho}
    (\mathbf{A}_m, \mathbf{B}_m,u_{0,m-1},\mathrm{Tol},u_{0,n},\dots
    ,u_{j-1,m-1})$}


\ENDFOR
\STATE{$(\Lambda_m,U_m):=\mathrm{Reconstruction}
    (\{(\lambda_{\mathrm{DTE}+j,m},u_{\mathrm{DTE}+j,m})\}_{j=0}^{\mathrm{FIE}-1},
(\Lambda_{m-1},U_{m-1}))$}
\STATE{Compute the residual $\eta$ for the couple $(\Lambda_m,U_m)$}
\STATE{$m:=m+1$}
\UNTIL{ $m\leq M$ OR $\eta \leq \mathrm{err}$}
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A priori convergence results}\label{sse:pcf_priori}

%\textcolor{red}{This section is for smooth problems only, i.e. $u\in H^s(\Omega)$, $s\ge 2$.}

In this section  we gather together some a priori estimates for eigenvalue
problems.  These results are mostly classical, suitable references are 
\cite{BaOs:87,BaOs:89,babuska,strang}.






It follows from the coercivity of the bilinear form $a(\cdot,\cdot)$ that
all eigenvalues of  \eqref{eq:var_prob} and all $N=\dim V_n$
eigenvalues of \eqref{eq:disc_prob} are positive.
We can order
them as $0 < \lambda_1 \leq \lambda_2 \ldots $ and $0 < \lambda_{1,n}
\leq \lambda_{2,n} \ldots \leq \lambda_{N,n}$. Moreover, we know (e.g. 
\cite{BaOs:89}) that  $\lambda_{j,n} \rightarrow \lambda_j$,
for any
$j$,  as  $V_n
\rightarrow H^1(\Omega)$ and (by the minimax principle) 
that $\lambda_{j,n}$ is monotone
non-increasing, i.e.
\begin{equation}\label{eq:minimax_shift}
\lambda_{j,n} \ \geq\  \lambda_{j,m}\  \geq\   \lambda_j \ , \quad
\text{for all} \quad j = 1, \ldots , N, \quad \text{and all} \quad
m \geq n \ .
\end{equation}

The distance of an approximate eigenfunction from the true eigenspace
is a crucial quantity in the convergence analysis for
eigenvalue problems  especially in the case of non-simple
eigenvalues.

\begin{definition}
\label{def:dist_l2}
Given a function $v\in L^2(\Omega)$ and a finite dimensional subspace $\mathcal{P}\subset L^2(\Omega)$, we define:
$$
\mathrm{dist}(v,\mathcal{P})_{0,\cB}\ :=\ \min_{ w\in\mathcal{P}}  \|v-w\|_{0} \ .
$$
Similarly, given a function $v\in H^1_\pi(\Omega)$ and a finite dimensional subspace $\mathcal{P}\subset H^1_\pi(\Omega)$, we define:
$$
\mathrm{dist}(v,\mathcal{P})_{1}\ :=\ \min_{ w\in\mathcal{P}}  \|v-w\|_{1} \ .
$$
\end{definition}

Now let $\lambda_j$ be any eigenvalue of 
\eqref{eq:var_prob},  let $E(\lambda_j)$ denote the (finite
dimensional) space spanned by  the eigenfunctions of  $\lambda_j$ and set
$E_1(\lambda_j)=\{u\in E(\lambda_j):
\|u\|_{0}=1\}$. 
{Let $T_{\lambda_j}$
  denote the orthogonal projection of $H^1$ onto $E(\lambda_j)$ with respect
  to the inner product $a(\cdot, \cdot)$.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{lemma}\label{lm:inf_l2_h1}
 Let $(\lambda_{j,n},u_{j,n})$ be an eigenpair of \eqref{eq:disc_prob}. Then
\begin{equation}\label{eq:inf_l2_h1_1}
\|u_{j,n}-u_j\|_{0} = \mathrm{dist}(u_{j,n},E_1(\lambda_j))_{0}\ ,
\end{equation}
if and only if
\begin{equation}\label{eq:inf_l2_h1_2}
\|{u_{j,n}-u_j}\|_{1}=\mathrm{dist}(u_{j,n},E_1(\lambda_j))_{1}\ .
\end{equation}

\end{lemma}

\begin{proof}
%\ednote{It is shortened - please read it carefully}
{Since $E(\lambda_j)$ is
finite dimensional,   
the minimizers in \eqref{eq:inf_l2_h1_1} and \eqref{eq:inf_l2_h1_2}
exist. Moreover 
\begin{equation}\label{eq:l2_ortho_1}
0  \ = \ a(T_{\lambda_j} w, (I-T_{\lambda_j}) v) \ =\
\lambda_j\ b(T_{\lambda_j} w, (I-T_{\lambda_j}) v) \   \quad \text{for all} \quad 
v,w\in L^2_\cB(\Omega)\cap H_{\pi}^1(\Omega)\ .
\end{equation}
Hence for any $v_j \in E(\lambda_j)$ we have the decomposition   
$$u_{j,n}-v_j\ = \ (I-T_{\lambda_j})u_{j,n}\ +\ T_{\lambda_j} (u_{j,n}-v_j)
\ = \  (I-T_{\lambda_j})u_{j,n}\ +\ (T_{\lambda_j} u_{j,n}-v_j) \ ,
$$
which is orthogonal both with respect to $a(\cdot, \cdot)$
and $b(\cdot, \cdot)$. Thus 
\begin{eqnarray*}
\|u_{j,n}-v_j\|_{0}^2\ & = & \ 
\|(I-T_{\lambda_j})u_{j,n}\|_{0}^2 +
\|T_{\lambda_j} u_{j,n}-v_j\|_{0}^2 \ \ ,\\
\|u_{j,n}-v_j\|_{1}^2\ & = & \ 
\|(I-T_{\lambda_j})u_{j,n}\|_{1}^2 +
\|T_{\lambda_j} u_{j,n}-v_j\|_{1}^2 \ .
\end{eqnarray*}
Hence $u_j$  satisfies \eqref{eq:inf_l2_h1_2}  if and only if it minimizes 
$\|T_{\lambda_j}u_{j,n}-v_j\|_{1}^2$.  The latter quantity is
equal to \\$\lambda_j \|T_{\lambda_j}u_{j,n}-v_j\|_{0}^2$
and hence $u_j$ satisfies  \eqref{eq:inf_l2_h1_2} if and only
if it satisfies \eqref{eq:inf_l2_h1_1}.}

\end{proof}

Let $u_j$ and $u_{j,n}$ be any
normalized eigenvectors of \eqref{eq:var_prob}
and \eqref{eq:disc_prob}.
Then
\begin{eqnarray}
\label{eq:basic1} a(u_j - u_{j,n}, u_j - u_{j,n}) &=& a(u_j,u_j) +
a(u_{j,n},u_{j,n})
- 2 a(u_{j},u_{j,n})\nonumber\\
&=&    \lambda_j +  \lambda_{j,n} -  2\lambda_j \ b(u_{j},u_{j,n})
\nonumber\\
&=&      (\lambda_{j,n} - \lambda_j)  +2  \lambda_j\ (1-b(u_{j},u_{j,n}))\nonumber\\
&=&
(\lambda_{j,n} - \lambda_j)  +\lambda_j\
b(u_{j}-u_{j,n},u_{j}-u_{j,n} ) \ .
\end{eqnarray}
Combining this with \eqref{eq:minimax_shift}, we obtain
\begin{equation}
a(u_j-u_{j,n},u_j-u_{j,n}) \ =\ |a(u_j-u_{j,n},u_j-u_{j,n})|\ =\  |\lambda_j-\lambda_{j,n}| \ + \
\lambda_j \ \Vert u_{j}-u_{j,n}\Vert_{0,\cB}^2\ .
\label{eq:basic2}
\end{equation}

In order to make further progress we need some assumption on
regularity of solutions of elliptic problems associated with $a(\cdot,
\cdot)$.
%\textcolor{red}{If we are going to consider non-smooth problems, I need to change this assumption.}
\begin{assumption}\label{ass:ell}
 We assume that there exists a constant
$C_\mathrm{ell}>0$ with the following property.
For   $f \in L^2(\Omega)$ and with the solution operator $\cS$, we have that if  $v: = \cS f\in H^1_0(\Omega)$ solves  the
problem {$a(v,w) = b(f,w) $} for all $w \in
H^1_0(\Omega)$, then 
\begin{equation}\label{eq:ass_reg_pcf}
\Vert \cS f \Vert_{{2}} \leq
C_\mathrm{ell}\Vert f \Vert_0\ ,
\end{equation}
where  $\Vert \cdot \Vert_{2}$ is the norm in   the Sobolev space $H^{2}(\Omega)$.
%\begin{equation}\label{eq:ass_reg_pcf}
%\Vert \mathcal{S} f \Vert_{{1+s}} \leq
%C_\mathrm{ell}\Vert f \Vert_0\ ,
%\end{equation}
%where  $\Vert \cdot \Vert_{{1+s}}$ is the norm in   the Sobolev space $H^{1+s}(\Omega)$.
\end{assumption}
This is a standard assumption which is satisfied in a wide number of
applications such as problems with discontinuous coefficients
(see eg. \cite{conv_sinum} for more references).\\

From now on we shall let $C$ denote  a generic constant which 
may depend
on the 
true eigenvalues and vectors of \eqref{eq:var_prob} and other
constants introduced above, but is always independent of
$n$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\textcolor{red}{This theorem must be changed if we allow for non-smooth problems}
\begin{lemma}
\label{lm:adj}
Suppose  $ 1 \leq j\leq \dim V_n$. Let
$\lambda_j$ be an eigenvalue   of \eqref{eq:var_prob} with
corresponding eigenspace $E(\lambda_j)\subset H^{1+\mu}(\Omega)$, for $\mu>1$, of any (finite) dimension  and
let $(\lambda_{j,n},u_{j,n})$ be an  eigenpair  of \eqref{eq:disc_prob}.
Then, for a finite element space $V_n$ sufficiently rich,
\begin{itemize}
\item[(i)] 
\begin{equation}
\vert \lambda_j - \lambda_{j,n} \vert \ \leq \ (\mathrm{dist}(
u_{j,n},E_1(\lambda_j))_{1})^2; \quad \text{and} \quad
\vert \lambda_j - \lambda_{j,n} \vert \ \leq \ C
\frac{h_n^{2\mu} }{p_n^{2\mu}};  \label{eq:supereig}
\end{equation}
\item[(ii)] 
\begin{eqnarray}
\mathrm{dist}(
u_{j,n},E_1(\lambda_j))_{0}\ & \leq& \ C \frac{h_n}{p_n}
 \mathrm{dist}(
u_{j,n},E_1(\lambda_j))_{1} \ ; \label{eq:adj}
\end{eqnarray}
%\begin{eqnarray}
%\mathrm{dist}(
%u_{j,n},E_1(\lambda_j))_{0}\ & \leq& \ C \frac{h_n^s}{p_n^s}
% \mathrm{dist}(
%u_{j,n},E_1(\lambda_j))_{1} \ ; \label{eq:adj}
%\end{eqnarray}
\item[(iii)]
\begin{equation}
\label{eq:energy} \mathrm{dist}(
u_{j,n},E_1(\lambda_j))_{1} \ \leq
C \frac{h_n^{\mu}}{p_n^{\mu}}\ , 
\end{equation}
\end{itemize}
with $1\leq \mu\leq p_n$
\end{lemma}

\begin{proof}\
First consider part (i). 
Since $\lambda_j \geq  0$, the first estimate in 
\eqref{eq:supereig} follows directly from \eqref{eq:basic2}.
 To obtain the second estimate in \eqref{eq:supereig},  
we recall a  standard error  estimate for elliptic eigenvalues   
(see e.g.  \cite[(1.1)]{BaOs:89}) which gives  
$$ \lambda_{j,n} - \lambda_j \ \leq \  C \sup_{u \in
  E_1(\lambda_j)} \inf_{v_n \in V_n} \Vert u - v_n \Vert_1^2\ . $$
Combining this with standard finite element error
estimates for $hp$-method, see \cite[Theorem~4.72]{schwab} and recalling \eqref{eq:minimax_shift}, we get  
\begin{eqnarray}
\vert \lambda_{j,n} - \lambda_j \vert \  \  
\ \leq \ C \frac{h_n^{2\min(\mu,p)} }{p_n^{2\mu}} \sup_{u \in
  E_1(\lambda_j)} \Vert u \Vert_{1+\mu}^2 ,  \label{eq:second_est} \end{eqnarray}
% For  $u
%\in E_1(\lambda_j)$, Assumption \ref{ass:ell} implies 
%%$\Vert u \Vert_{1+s} \ \leq \ C_{ell} \lambda_j  \Vert u
%%\Vert_{0}  \ \leq \ C_{ell} \lambda_j$
%$\Vert u \Vert_2 \ \leq \ C_{ell} \lambda_j \Vert u
%\Vert_{0}  \ \leq \ C_{ell} \lambda_j$,  which yields the
%result. 

To obtain  (ii),  we use the following estimate 
\cite[(3.31a)]{BaOs:89}:
\begin{equation}\label{eq:BaOs}\frac{\Vert T_{\lambda_j}u_{j, n} - u_{j, n}
  \Vert_{0}}{\Vert T_{\lambda_j}u_{j, n} - u_{j, n}
  \Vert_{1}} \ \leq \ C \eta_n\ , \quad 
\text{where} \quad \eta_n \ = \ \sup_{\stackrel{g \in L^2(\Omega)}{\Vert
    g\Vert_{0} = 1 }} \inf_{\chi \in V_n} \Vert \cS g - \chi
\Vert_{1} \ , \end{equation} and $\cS $ is the solution
operator defined in
Assumption \ref{ass:ell}. Analogously to \eqref{eq:second_est} and with the further restriction that $\cS g\in H^2(\Omega)$ only, we have
$\eta_n \leq C h_n p_n^{-1}$ and hence \eqref{eq:BaOs} implies
 \begin{eqnarray}
\Vert T_{\lambda_j}u_{j, n} - u_{j, n}
  \Vert_0 \ & \leq  & \  C     \frac{h_n}{p_n} \Vert T_{\lambda_j}u_{j, n} - u_{j, n}
  \Vert_1  \nonumber \\
& =  & \ C \frac{h_n}{p_n} \mathrm{dist} (u_{j,n}
,E(\lambda_j))_1  \nonumber  \\ 
& \leq & \  C   \frac{h_n}{p_n} \mathrm{dist} (u_{j,n}
,E_1(\lambda_j))_1 \ ,  \label{eq:new2}
 \end{eqnarray} 
%  \begin{eqnarray}
%\Vert T_{\lambda_j}u_{j, n} - u_{j, n}
%  \Vert_0 \ & \leq  & \  C     \frac{h_n^s}{p_n^{s-1}} \Vert T_{\lambda_j}u_{j, n} - u_{j, n}
%  \Vert_1  \nonumber \\
%& =  & \ C \frac{h_n^s}{p_n^{s-1}} \mathrm{dist} (u_{j,n}
%,E(\lambda_j))_1  \nonumber  \\ 
%& \leq & \  C   \frac{h_n^s}{p_n^{s-1}} \mathrm{dist} (u_{j,n}
%,E_1(\lambda_j))_1 \ ,  \label{eq:new2}
% \end{eqnarray} 
where we used the inclusion $E_1(\lambda_j) \subset E(\lambda_j)$. 
Since $\Vert u_{j,n}\Vert_0 = 1$,  \eqref{eq:new2} also implies
that 
 \begin{eqnarray}
\bigg\vert \Vert T_{\lambda_j}u_{j, n} \Vert_0  -1 \bigg\vert \
& \leq  & \  \Vert T_{\lambda_j}u_{j, n} - u_{j, n}
  \Vert_0  \nonumber  \\
& \leq & \  C   \frac{h_n}{p_n} \mathrm{dist} (u_{j,n}
,E_1(\lambda_j))_1 \ . \label{eq:new4}
 \end{eqnarray} 
%  \begin{eqnarray}
%\bigg\vert \Vert T_{\lambda_j}u_{j, n} \Vert_0  -1 \bigg\vert \
%& \leq  & \  \Vert T_{\lambda_j}u_{j, n} - u_{j, n}
%  \Vert_0  \nonumber  \\
%& \leq & \  C   \frac{h_n^s}{p_n^{s-1}} \mathrm{dist} (u_{j,n}
%,E_1(\lambda_j))_1 \ . \label{eq:new4}
% \end{eqnarray} 
Combining \eqref{eq:new2} and  \eqref{eq:new4},  we obtain
\begin{eqnarray*} 
\mathrm{dist}(u_{j,n}, E_1(\lambda_j))_0 \ & \leq & \ 
\bigg\Vert \frac{T_{\lambda_j}u_{j, n}}{\Vert T_{\lambda_j}u_{j, n}\Vert_0} - u_{j, n}
  \bigg\Vert_0 \\
& \leq & \ 
\bigg\Vert {T_{\lambda_j}u_{j, n}} - u_{j, n}
  \bigg\Vert_0 + \bigg\vert 1 - \Vert T_{\lambda_j}u_{j,n}\Vert_0^{-1}\bigg\vert  \ \Vert T_{\lambda_j}u_{j,n}\Vert_0\\
& =  & \ 
\bigg\Vert {T_{\lambda_j}u_{j, n}} - u_{j, n}
  \bigg\Vert_0 + \bigg\vert  \Vert T_{\lambda_j}u_{j,n}\Vert_0 - 1 \bigg\vert 
\\
& \leq & \ C \frac{h_n}{p_n} \mathrm{dist} (u_{j,n} \ ,
%& \leq & \ C \frac{h_n^s}{p_n^{s-1}} \mathrm{dist} (u_{j,n} \ ,
E_1(\lambda_j))_1 \ .
\end{eqnarray*}
which is \eqref{eq:adj}. 

Finally,  for part (iii),  we note that 
\eqref{eq:basic2},  Lemma \ref{lm:inf_l2_h1} and  \eqref{eq:supereig}
imply ,
\begin{equation}
\mathrm{dist}(u_{j,n}, E_1(\lambda_j))_1^2 \ \leq\  C
\frac{h_n^{\mu}}{p_n^{\mu}} \ + \ \lambda_j\,  \mathrm{dist}(u_{j,n}, E_1(\lambda_j))_0^2
\end{equation}
which, via \eqref{eq:adj}, implies  \eqref{eq:energy}.


\end{proof}


%\textcolor{red}{This result is for h-adaptive method, not for hp. I need to update it.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{theorem}
\label{th:adj_rec}
Suppose  $ 1 \leq j\leq \dim V_n$. Let
$\lambda_j$ be an eigenvalue   of \eqref{eq:var_prob} with
corresponding eigenspace $E(\lambda_j)$ of any (finite) dimension  and
let $(\Lambda_n,U_n)$ be a reconstructed eigenpair  of \eqref{eq:disc_prob}.
Then, for a finite element space $V_n$ sufficiently rich,
\begin{itemize}
\item[(i)] 
\begin{equation}
%\vert \lambda_j - \Lambda_n \vert \ \leq \ (\mathrm{dist}(
%U_n,E_1(\lambda_j))_{1})^2; \quad \text{and} \quad
\vert \lambda_j - \Lambda_n \vert \ \leq \ C
\frac{h_n^{2\mu} }{p_n^{2\mu}} ;  \label{eq:supereig_rec}
\end{equation}
\item[(ii)] 
\begin{eqnarray}
\mathrm{dist}(
U_n,E_1(\lambda_j))_{0}\ & \leq& \ C \frac{h_n^{\mu+1} }{p_n^{\mu+1}} \ ; \label{eq:adj_rec}
\end{eqnarray}
\item[(iii)]
\begin{equation}
\label{eq:energy_rec} \mathrm{dist}(
U_n,E_1(\lambda_j))_{1} \ \leq
C \frac{h_n^{\mu}}{p_n^{\mu}}\ ,
\end{equation}
\end{itemize}
with $1\leq \mu\leq p_n$
\end{theorem}

\begin{proof}
Recalling \eqref{eq:const}, let us denote for each $i$ by $u_i\in E(\lambda_j)$ the eigenfunctions that minimize both $\mathrm{dist}(
u_{i,n},E_1(\lambda_j))_{0}$ and $\mathrm{dist}(
u_{i,n},E_1(\lambda_j))_{1}$. Then denoting by 
$$
U=\sum_{i=1}^{R} c_i \ u_{i}\ ,
$$
we have that
$$
\mathrm{dist}(
U_n,E_1(\lambda_j))_{0}\leq \|U-U_n\|_{0}\leq \sum_{i=1}^{R} \mathrm{dist}(
u_{i,n},E_1(\lambda_j))_{0}\ ,
$$
and similarly we have
$$
\mathrm{dist}(
U_n,E_1(\lambda_j))_{0}\leq \|U-U_n\|_{1}\leq \sum_{i=1}^{R} \mathrm{dist}(
u_{i,n},E_1(\lambda_j))_{1}\ .
$$
Then results (ii) and (iii) comes straightforwardly form Lemma~\ref{lm:adj}(ii-iii).


By construction we have that $\Lambda:=\sum_{i=1}^{R} c_i^2 \ \lambda_{i,n}$ and from the minimum-maximum principle we have that for all $i$ $\lambda_j - \lambda_{i,n}\leq 0$
So from 
\eqref{eq:const} we have
$$
\sum_{i=1}^{R} c_i^2 \ \vert \lambda_j - \lambda_{i,n} \vert=
 \sum_{i=1}^{R} c_i^2 \ (\lambda_{i,n} - \lambda_j )=  \Lambda_n - \lambda_j
 = \vert \lambda_j - \Lambda_n \vert\ ,
$$
since it is also clear that $\Lambda_n\ge \lambda_j$. Then \eqref{eq:supereig_rec} come directly from 
Lemma~\ref{lm:adj}(i).

%$$
%\vert \lambda_j - \Lambda_n \vert \leq \sum_{i=1}^{R} c_i^2 \ \vert \lambda_j - \lambda_{i,n} \vert
%$$

\end{proof}


%\section{Adaptive $hp$-FEM for a Single Eigenfunction}



%\section{Computing Reference Solution via Picard's Method}



%\section{Computing Reference Solution via Newton's Method}



\section{Adaptive Multimesh $hp$-FEM for Simultaneous Approximation of Several Eigenfunctions}


\section{Numerical Results}

One of them should be a comparison of the multimesh $hp$-FEM vs. single-mesh 
$hp$-FEM on some problem with no repeated eigenvalues, both in terms of DOF and 
CPU time.

\section{Reproducibility of Results} \label{sec:reproducibility}

\section*{Acknowledgment}

The first author was supported by the Subcontract No. 00089911 of Battelle Energy
Alliance (U.S. Department of Energy intermediary) as well as by the Grant No. 
IAA100760702 of the Grant Agency of the Academy of Sciences of the Czech Republic.


\clearpage

\begin{thebibliography}{[KLR73]}

\bibitem{dubcova1}
L. Dubcova, P. Solin, G. Hansen, H. Park: Comparison of Multimesh hp-FEM 
to Interpolation and Projection Methods for Spatial Coupling of Reactor 
Thermal and Neutron Diffusion Calculations, J. Comput. Phys. 230 (2011) 1182-1197.

\bibitem{giani1}
S. Giani, I.G. Graham: A convergent adaptive method for elliptic eigenvalue 
problems. SIAM J. Numer. Anal. 47 (2009), 1067-1091.

\bibitem{solin1} 
P. Solin, D. Andrs, J. Cerveny, M. Simko: 
PDE-Independent Adaptive $hp$-FEM Based on Hierarchic Extension of Finite Element Spaces.
J. Comput. Appl. Math. 233 (2010) 3086-3094.

\bibitem{solin2} 
P. Solin, J. Cerveny, L. Dubcova, D. Andrs: Monolithic Discretization of Linear 
Thermoelasticity Problems via Adaptive Multimesh hp-FEM, J. Comput. Appl. Math 
234 (2010) 2350-2357.

\bibitem{solin3}
P. Solin, K. Segeth, I. Dolezel: {\em Higher-Order Finite Element Methods},
Chapman \& Hall / CRC Press, 2003.

\bibitem{BaOs:87}
I.~Babu\v{s}ka and J.~Osborn.
\newblock {\em Estimates for the errors in eigenvalue and eigenvector
  approximation by Galerkin methods, with particular attention to the
  case of multiple eigenvalues}, 
\newblock{\em SIAM J. Numer. Anal.}, 24:1249-1276, 1987.

\bibitem{BaOs:89}
I.~Babu\v{s}ka and J.~Osborn.
\newblock {\em Finite element-Galerkin approximation of the
  eigenvalues and eigenvectors of selfadjoint problems}, 
\newblock{\em Math. Comput.} 186:275-297, 1989.
 
\bibitem{strang}
G.~Strang and G.~J. Fix.
\newblock {\em An Analysis of the Finite Element Method}.
\newblock Prentice-Hall, 1973.

\bibitem{babuska}
I.~Babu\v{s}ka and J.~Osborn.
\newblock {\em Eigenvalue Problems}.
\newblock in Handbook of Numerical Analysis Vol II, eds P.G. Cairlet and J.L. Lions, North Holland, 1991.

\bibitem{hackbusch}
W.~Hackbusch.
\newblock {\em Elliptic Differential Equations}.
\newblock Springer, 1992.

\bibitem{conv_sinum}
S. Giani and Ivan G. Graham,
\newblock {A Convergent Adaptive Method for Elliptic Eigenvalue Problems},
\newblock{\em SIAM J. Numer. Anal.} 47(2):1067-1091, 2009.

\bibitem{schwab}
C. Schwab,
\newblock {$p$- and $hp$- finite element methods},
\newblock{\em Oxford University Press}, 1998.


\end{thebibliography}



\end{document}

%%
%% End of file `elsarticle-template-num.tex'.
